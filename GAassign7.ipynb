{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebad8840-af25-4b62-8fd9-b7dd651893df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\tejaswini chandargi\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tejaswini chandargi\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 9.3 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "#Group A-7\n",
    "# Text Analytics\n",
    "# 1. Extract Sample document and apply following document preprocessing methods:\n",
    "# Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "# 2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "# Frequency.\n",
    "#nltk stands for Natural Language Toolkit, and it is a popular Python library used for working with human language data (i.e., text). It's a toolkit that rovides tools for tasks like text processing, classification, tokenization,stemming, lemmatization, and much more.\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "603d3759-0fec-4857-8ec1-391449bf617f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing (NLP) enables machines to understand and interpret human language. It bridges the gap between computers and humans by analyzing, tokenizing, and processing textual data. Techniques like stemming and lemmatization play a crucial role in simplifying text data while retaining its meaning. NLP is widely used in applications such as chatbots, sentiment analysis, and search engines.\n"
     ]
    }
   ],
   "source": [
    "#Open Text File\n",
    "text_file=open(\"text.txt\")\n",
    "text=text_file.read()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e8e598-567d-4315-8e76-019aa3d7378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\TEJASWINI\n",
      "[nltk_data]     CHANDARGI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The punkt package is a pre-trained tokenizer model that helps with splitting text into sentences or words.\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b5e62bb-cdd8-4a43-aa33-6212d25718f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\TEJASWINI\n",
      "[nltk_data]     CHANDARGI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5c2eaec-596f-4cd3-bf55-a6b263fe0c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing (NLP) enables machines to understand and interpret human language.', 'It bridges the gap between computers and humans by analyzing, tokenizing, and processing textual data.', 'Techniques like stemming and lemmatization play a crucial role in simplifying text data while retaining its meaning.', 'NLP is widely used in applications such as chatbots, sentiment analysis, and search engines.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization is the process of splitting a text into smaller units called tokens. These tokens can be words, subwords, sentences, or characters, depending on the type of tokenization. In Natural Language Processing (NLP), tokenization helps break down text into manageable parts for further analysis or processing.\n",
    "#For example: Text: \"Hello world!\" Tokens: [\"Hello\", \"world\", \"!\"] Tokenization is a crucial first step in many NLP tasks, like text classification, sentiment analysis, and language modeling.\n",
    "#Sentence Tokenization\n",
    "#The sent_tokenize function splits the text into sentences based on punctuation marks like periods, question marks, and exclamation marks.In this example, the text \"Hello! How are you doing today? NLTK is a great tool for NLP tasks.\" gets split into 3 sentences.\n",
    "#['Hello!', 'How are you doing today?', 'NLTK is a great tool for NLP tasks.']\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "sentence_tokenized = sent_tokenize(text)\n",
    "print(sentence_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4797d8d4-3d4d-445b-bb1d-d3ea6f5b9aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'enables', 'machines', 'to', 'understand', 'and', 'interpret', 'human', 'language', '.', 'It', 'bridges', 'the', 'gap', 'between', 'computers', 'and', 'humans', 'by', 'analyzing', ',', 'tokenizing', ',', 'and', 'processing', 'textual', 'data', '.', 'Techniques', 'like', 'stemming', 'and', 'lemmatization', 'play', 'a', 'crucial', 'role', 'in', 'simplifying', 'text', 'data', 'while', 'retaining', 'its', 'meaning', '.', 'NLP', 'is', 'widely', 'used', 'in', 'applications', 'such', 'as', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'search', 'engines', '.']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "word_tokenized = word_tokenize(text)\n",
    "print(word_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32063b08-6375-44f0-9f0f-2b29e03623bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 5), ('.', 4), (',', 4), ('NLP', 2), ('data', 2)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Frequency of words\n",
    "from nltk.probability import FreqDist\n",
    "text_tokenized = word_tokenize(text)\n",
    "fdist=FreqDist(text_tokenized)\n",
    "fdist.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3437372-1817-4b18-93c8-7d6f8d6c2b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural: 1\n",
      "Language: 1\n",
      "Processing: 1\n",
      "(: 1\n",
      "NLP: 2\n",
      "): 1\n",
      "enables: 1\n",
      "machines: 1\n",
      "to: 1\n",
      "understand: 1\n",
      "and: 5\n",
      "interpret: 1\n",
      "human: 1\n",
      "language: 1\n",
      ".: 4\n",
      "It: 1\n",
      "bridges: 1\n",
      "the: 1\n",
      "gap: 1\n",
      "between: 1\n",
      "computers: 1\n",
      "humans: 1\n",
      "by: 1\n",
      "analyzing: 1\n",
      ",: 4\n",
      "tokenizing: 1\n",
      "processing: 1\n",
      "textual: 1\n",
      "data: 2\n",
      "Techniques: 1\n",
      "like: 1\n",
      "stemming: 1\n",
      "lemmatization: 1\n",
      "play: 1\n",
      "a: 1\n",
      "crucial: 1\n",
      "role: 1\n",
      "in: 2\n",
      "simplifying: 1\n",
      "text: 1\n",
      "while: 1\n",
      "retaining: 1\n",
      "its: 1\n",
      "meaning: 1\n",
      "is: 1\n",
      "widely: 1\n",
      "used: 1\n",
      "applications: 1\n",
      "such: 1\n",
      "as: 1\n",
      "chatbots: 1\n",
      "sentiment: 1\n",
      "analysis: 1\n",
      "search: 1\n",
      "engines: 1\n"
     ]
    }
   ],
   "source": [
    "#Frequency of words\n",
    "from nltk.probability import FreqDist\n",
    "text_tokenized = word_tokenize(text)\n",
    "fdist=FreqDist(text_tokenized)\n",
    "all_words = fdist.items()\n",
    "for word, count in all_words:\n",
    " print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4ce3b6a-c695-4b10-9bcf-eecf0a318862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world This is a test does it work\n"
     ]
    }
   ],
   "source": [
    "#Remove Punctuation\n",
    "from string import punctuation\n",
    "punctuation\n",
    "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "import string\n",
    "# Define the text variable\n",
    "text = \"Hello, world! This is a test: does it work?\"\n",
    "# Remove punctuation from the text\n",
    "text_clean = \"\".join([char for char in text if char not in\n",
    "string.punctuation])\n",
    "print(text_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd1b2cb1-9697-4eeb-ad96-71da0df198a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Natural Language Processing (NLP) enables machines to understand and interpret human language. It bridges the gap between computers and humans by analyzing, tokenizing, and processing textual data. Techniques like stemming and lemmatization play a crucial role in simplifying text data while retaining its meaning. NLP is widely used in applications such as chatbots, sentiment analysis, and search engines.\n",
      "Stopwords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "Text without stopwords: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'enables', 'machines', 'understand', 'interpret', 'human', 'language', '.', 'bridges', 'gap', 'computers', 'humans', 'analyzing', ',', 'tokenizing', ',', 'processing', 'textual', 'data', '.', 'Techniques', 'like', 'stemming', 'lemmatization', 'play', 'crucial', 'role', 'simplifying', 'text', 'data', 'retaining', 'meaning', '.', 'NLP', 'widely', 'used', 'applications', 'chatbots', ',', 'sentiment', 'analysis', ',', 'search', 'engines', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\TEJASWINI\n",
      "[nltk_data]     CHANDARGI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Open the file and read the text\n",
    "with open(\"text.txt\", \"r\") as text_file:\n",
    " text = text_file.read()\n",
    "# Print the text to verify the content\n",
    "print(\"Original Text:\", text)\n",
    "# Tokenizing the text into words\n",
    "text_tokenized = word_tokenize(text)\n",
    "# Get the list of English stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "# Print the list of stopwords\n",
    "print(\"Stopwords:\", stopwords_list)\n",
    "# Remove stopwords from the tokenized text\n",
    "text_nostop = [word for word in text_tokenized if word.lower() not in\n",
    "stopwords_list]\n",
    "# Print the text without stopwords\n",
    "print(\"Text without stopwords:\", text_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e6ada03-0daf-481c-a827-40577149ccce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\TEJASWINI\n",
      "[nltk_data]     CHANDARGI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS TAGGING Parts of speech\n",
    "# POS tagging, or Part-of-Speech tagging, is the process of assigning a specific part of speech to each word in a sentence or text. Parts of speech are categories that words can be classified into based on their role in a sentence. Common parts of speech include:\n",
    "# Nouns (NN, NNS, etc.): Words that represent people, places, things, or concepts (e.g., \"dog,\" \"city\").\n",
    "# Verbs (VB, VBD, etc.): Words that represent actions or states (e.g., \"run\" , \"eat\").\n",
    "# Adjectives (JJ, JJR, etc.): Words that describe or modify nouns (e.g., \"beautiful,\" \"quick\").\n",
    "# Adverbs (RB, RBR, etc.): Words that describe or modify verbs, adjectives,or other adverbs (e.g., \"quickly,\" \"very\").\n",
    "# Pronouns (PRP, PRP$): Words that take the place of nouns (e.g., \"he,\" \"they\").\n",
    "# Prepositions (IN): Words that show relationships between other words (e.g., \"in,\" \"on,\" \"at\").\n",
    "# Conjunctions (CC): Words that connect other words, phrases, or clauses (e.g., \"and,\" \"but\").\n",
    "# Interjections (UH): Words that express emotions or sudden exclamations (e.g., \"wow,\" \"ouch\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72055e11-71fa-4b62-8742-289007aaa35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words: ['The', 'children', 'are', 'running', 'and', 'they', 'are', 'better', 'at', 'it', 'than', 'before', '.']\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "# Sample text text = \"The children are running and they are better at it than before.\"\n",
    "# Tokenizing the text into words\n",
    "text_token = word_tokenize(text)\n",
    "print(\"Tokenized Words:\", text_token)\n",
    "# POS tagging of the tokenized words\n",
    "post_tags = pos_tag(text_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "894a0c82-63e1-4640-a032-5237045731fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studi\n",
      "stude\n",
      "studi\n"
     ]
    }
   ],
   "source": [
    "# Stemming is the process of reducing a word to its base or root form by removing suffixes and prefixes. The goal is to simplify words to their core form, which can then be more easily analyzed in natural language processing (NLP) tasks.\n",
    "# For example:\n",
    "# \"better\" → \"better\" (sometimes, some words may not be reduced)\n",
    "# \"happiness\" → \"happi\"\n",
    "# Stemming algorithms typically use rules to chop off the end of words, though they may not always result in actual words (like \"happi\"). It is useful in text mining, search engines, and information retrieval, where you want to group similar words together (e.g., \"run,\" \"running,\" \"runner\" as a single stem \"run\").\n",
    "from nltk.stem import PorterStemmer\n",
    "# Initialize the stemmer\n",
    "ps = PorterStemmer()\n",
    "# Example words\n",
    "words = ['study', 'studing', 'studies']\n",
    "# Stemming the words\n",
    "for word in words:\n",
    " print(ps.stem(word))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a76ba52-63db-4f15-94f9-f29ed0769848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\TEJASWINI\n",
      "[nltk_data]     CHANDARGI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\TEJASWINI\n",
      "[nltk_data]     CHANDARGI\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "#It is the process of converting a word to its base or root form. Unlike stemming, which simply chops off parts of words, lemmatization uses dictionaries to get the real root form.\n",
    "#It uses a dictionary (like WordNet) and sometimes requires part-of-speech (POS) tagging for more accuracy. It helps retain the original meaning of the word in text analysis.\n",
    "#For example:\"running\" becomes \"run\" \"better\" becomes \"good\" It focuses on the correct meaning of words, so \"am,\" \"is,\" and \"are\" might be reduced to \"be.\"\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a969dd8b-c1cc-4661-9027-981d7865833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['The', 'child', 'are', 'running', 'and', 'they', 'are', 'better', 'at', 'it', 'than', 'before', '.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Sample sentence with different words text = \"The leaves are falling from the trees and the children are running quickly.\"\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "# Lemmatize each token\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "882b672a-f508-4c1d-b2a4-b0cc5a142c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tejaswini chandargi\\appdata\\roaming\\python\\python313\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\tejaswini chandargi\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\tejaswini chandargi\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tejaswini chandargi\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tejaswini chandargi\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF which stands for Trem Frequency and inverse document frequency,is a crucial concept in NLP .It is a numerical represenatatiom that helps quantify the importance of words in a collection of documents. it is a numerical represnetatiom that measures the importance of words in a document\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5baece8e-5c33-4a32-a573-ad8b7745e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample list of documents\n",
    "doc_1 = \"This is a sample document.\"\n",
    "doc_2 = \"This document is another example.\"\n",
    "doc_3 = \"TF-IDF is useful for text processing.\"\n",
    "# Store documents in a list\n",
    "documents = [doc_1, doc_2, doc_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eddf061b-db92-4412-9efd-bf596ca73dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.         0.4804584  0.         0.         0.         0.37311881\n",
      "  0.         0.63174505 0.         0.         0.4804584  0.        ]\n",
      " [0.53409337 0.40619178 0.53409337 0.         0.         0.31544415\n",
      "  0.         0.         0.         0.         0.40619178 0.        ]\n",
      " [0.         0.         0.         0.39687454 0.39687454 0.2344005\n",
      "  0.39687454 0.         0.39687454 0.39687454 0.         0.39687454]]\n",
      "\n",
      "Feature Names:\n",
      "['another' 'document' 'example' 'for' 'idf' 'is' 'processing' 'sample'\n",
      " 'text' 'tf' 'this' 'useful']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the documents to calculate TF-IDF scores\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents) #fit means providing data to algorithm and tfidf vectorizer will transform the result\n",
    "# Convert the matrix to an array and print\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "# Get the feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e9ea6-02f9-4b9f-bb2a-59be71ac22ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
